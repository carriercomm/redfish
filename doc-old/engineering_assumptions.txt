ENGINEERING ASSUMPTIONS
* The network is composed of racks of 20 to 40 machines.
Machines in a single rack can talk to each other cheaply over gigabit.
Inter-rack communication is more expensive and goes over a single, shared 4
gigabit or 10 gigabit switch.

* Because of the network topology, backing up the data on the clusters is
simply impossible. There is not enough bandwidth out to make it practical.
That means we need an extremely robust filesystem that is stable and
self-healing enough not to need periodic backups to tape.

* Files are generally many megabytes in size or bigger.

* High bandwidth is the main focus, rather than ultra-low latency.

* We support HDFS semantics rather than POSIX semantics
* HDFS semantics:
        * Multiple clients cannot write to the same file at once (No "random writers")
        * Files are only guaranteed to be readable by other clients once the
writer has closed the file, or called hflush and gotten a response.
        * No hardlinks
        * No softlinks
        * Instead of readdir/seekdir/telldir, we have a command that simply
retrieves a complete list of files in the current directory. Optionally,
filters can be specified to reduce the number of entries returned.

* Single points of failure are unacceptable.

* Limiting the amount of metadata to the disk size or memory size of a single
machine is unacceptable.

