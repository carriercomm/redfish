SHARDING
* MDSes are organized into MDS shards.
For example, if A, B, C, and D are MDS nodes, we might have something like this:
Shard 1: A B
Shard 2:   B C
Shard 3:     C D
Shard 4: A     D

We use full-path hashing to assign each directory and each file to a single shard.
For each shard, there is exactly one primary MDS and the rest are standby
MDSes. Realistically, shards are likely to consist of two or three MDSes.

We use consistent hashing so that only K/n keys need to remapped on average
when a new shard is added or an old shard removed. Here, K is the number of
keys and n the number of slots.

Clients send modification operations to the primary MDS in a shard.
Clients send view operations to a random MDS in a shard.

ACCESS CONTROL
Access control management is the bane of clustered filesystems that shard on
full paths. For example, in order to successfully open /a/b/c, you must have
list permission on /a and /a/b. We could ask the MDSes that are responsible
for /a and /a/b whether permissions is granted, but that would be slow.

However, ACL information can be cached, as it changes very infrequently. We
can store the ACLs of all ancestor directories along with each file or
directory. An ACL change operation simply applies to all of these.

MODIFICATION OPERATIONS
The primary MDS for the shard receives the modification operation.
It assigns it a new modification operation ID (mod_id) from an incrementing
64-bit counter.
* The primary tries to perform the operation.
* If it fails, it sends back the error code to the client.
* If it succeeds, it sends the operation to the next MDS in the shard. This MDS
applies the operation and sends it on the next MDS after that. The final MDS
in the shard sends a confirmation response back to the client.
* If any auxillary MDSes drop the message, or are down, the client will not
get a response. The client will send a status request about the operation, and
the primary will use a two-phase commit to check that all replicas are alive
and have applied it.
* Also, the auxillary MDSes will include "acked up to X in shard Y"
information in their periodic heartbeats.

VIEW OPERATIONS
View operations are serviced by the MDS that receives the view operation.
If an operation has already been applied on the primary, but not yet mirrored
on the auxillary, there is a window of time when the auxillary could give out
stale data. This is not a problem.

INTER-SHARD MODIFICATION OPERATIONS
There are two inter-shard modification operations: renaming, and
changing a directory ACL.

Renaming is handled by the source shard asking the target shard to do the
operation. If the operation is not possible, the operation returns failure.

If it is possible, we initiate a "recursive" process that propagates down the
directory hierarchy. One level at a time, every primary MDS gives away the
entries at a certain level to the new shard.

Changing directory ACLs is handled similarly

HEARTBEATS
One of the biggest problems with any multi-MDS system is avoiding "split-brain
syndrome." This is where an MDS becomes unresponsive and is booted out of the
cluster, but then later revives itself. Clients may then start talking to the
zombie MDS and receive out-of-date information.

Here is how OneFish solves this problem.

Each MDS daemon keeps track of how recently it has sent messages to all the
other MDSes. If that value exceeds a certain time period (3 minutes or so), it
will send out a message to let the other MDS know that it is still alive.

Similarly, the MDS daemons keep track of how recently they have received
messages from every other MDS daemon. If that exceeds a certain period (7
minutes), the MDS will send out a 2 phase commit proposal to blacklist the
offending MDS.

If all others agree, the offender is blacklisted.
If another MDS disagrees, the offender is rehabilitated in the eyes of its
accuser.
If some MDSes don't respond, they are added to the blacklist proposal.

If an MDS ever finds itself proposing more than half of the other MDSes as
down, it decides that in fact, it is the one that is down, and blacklists
itself. If an MDS has gone more than 5 minutes since sending out hearbeats to
every other MDS, it blacklists itself.

Once an MDS has been blacklisted, an administrator must manually re-add the
MDS to the cluster.

DELETING FILES
* Similar to HDFS, delete and deleteRecursive simply move files to a location
in the special /.Trash directory. Then, we'll have cleanup jobs that
periodically run on MDSes to take out the trash.

ADVANTAGES
* NO msync/fsync/O_DIRECT is required, because all the nodes in a shard have
all of the changes in memory. We don't need to use write-ahead logging.

* We can do hot failover because all the nodes have the state ready and
waiting in memory.

* since the metadata is sharded, we can scale to Exobytes of data on the same
filesystem!

DISADVANTAGES
* We can't achieve POSIX semantics. Whether this is really a disadvantage
depends on the application, of course.

* If a huge number of MDSes create new files in the exact same directory, all
those requests will have to go through a single MDS-- the primary MDS for the
relevant shard.
(The equivalent operation for files, many writers to the same file, is not
possible, because Hadoop only supports a single writer per file.)

* We never replicate a file or directory across *all* of the MDSes in the
cluster, like we do with some other schemes. This is simpler and probably the
best approach in practice, but it does have some implications. Only SHARD_SIZE
MDSes will ever be servicing requests relevant to a particular shard.

* The sharding is static, and doesn't adapt to what is going on. This seems
unlikely to be a problem in practice, but again, it is worth noting here.
An adversary could construct a pathological workload, although a real-world
programmer is very, very unlikely to do so.

IMPLEMENTATION
* We are going to need a tree structure in memory on the MDS nodes. Probably
the best thing to do is to use a B-Tree whose nodes are allocated from an
mmapp'ed file. Rather than using O_DIRECT, we can simply rely on the kernel's
writeback mechanisms. This also allows us to have trees that are larger than
memory, since the kernel handles paging.

* We probably want something like a lock per node, though that's not entirely
clear.

* It might be best to use UDP for all messages. Sending out a UDP message has
less overhead than setting up a TCP session. TCP requires the 3-way handshake
and all that.
